{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y_ = zip(*[(tX[i], y[i]) for i in range(tX.shape[0]) if tX[i,22] >1 ])\n",
    "x, y_ = np.array(x), np.array(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1.,  1., ..., -1.,  1., -1.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 3., 2., ..., 3., 3., 2.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/149): loss=0.5\n",
      "Gradient Descent(1/149): loss=0.45612021505677264\n",
      "Gradient Descent(2/149): loss=0.43855005949206494\n",
      "Gradient Descent(3/149): loss=0.4311141816149872\n",
      "Gradient Descent(4/149): loss=0.4276198069177341\n",
      "Gradient Descent(5/149): loss=0.4256848784310072\n",
      "Gradient Descent(6/149): loss=0.42438711010689123\n",
      "Gradient Descent(7/149): loss=0.42336588329337765\n",
      "Gradient Descent(8/149): loss=0.42247793064362\n",
      "Gradient Descent(9/149): loss=0.42166474885918614\n",
      "Gradient Descent(10/149): loss=0.4209012609262828\n",
      "Gradient Descent(11/149): loss=0.4201757982356884\n",
      "Gradient Descent(12/149): loss=0.4194822226418834\n",
      "Gradient Descent(13/149): loss=0.41881678618858426\n",
      "Gradient Descent(14/149): loss=0.4181768554963824\n",
      "Gradient Descent(15/149): loss=0.4175603780927164\n",
      "Gradient Descent(16/149): loss=0.41696564830374794\n",
      "Gradient Descent(17/149): loss=0.41639119662856905\n",
      "Gradient Descent(18/149): loss=0.4158357316212337\n",
      "Gradient Descent(19/149): loss=0.41529810520044763\n",
      "Gradient Descent(20/149): loss=0.41477728920319373\n",
      "Gradient Descent(21/149): loss=0.41427235791092415\n",
      "Gradient Descent(22/149): loss=0.41378247415568375\n",
      "Gradient Descent(23/149): loss=0.4133068778407798\n",
      "Gradient Descent(24/149): loss=0.41284487625103633\n",
      "Gradient Descent(25/149): loss=0.4123958357770231\n",
      "Gradient Descent(26/149): loss=0.4119591748006523\n",
      "Gradient Descent(27/149): loss=0.41153435755595275\n",
      "Gradient Descent(28/149): loss=0.4111208888187278\n",
      "Gradient Descent(29/149): loss=0.41071830930548897\n",
      "Gradient Descent(30/149): loss=0.4103261916815872\n",
      "Gradient Descent(31/149): loss=0.40994413709370164\n",
      "Gradient Descent(32/149): loss=0.4095717721542438\n",
      "Gradient Descent(33/149): loss=0.4092087463155754\n",
      "Gradient Descent(34/149): loss=0.4088547295806748\n",
      "Gradient Descent(35/149): loss=0.40850941050434825\n",
      "Gradient Descent(36/149): loss=0.40817249444545656\n",
      "Gradient Descent(37/149): loss=0.40784370203611536\n",
      "Gradient Descent(38/149): loss=0.407522767838529\n",
      "Gradient Descent(39/149): loss=0.407209439164182\n",
      "Gradient Descent(40/149): loss=0.40690347503358787\n",
      "Gradient Descent(41/149): loss=0.4066046452578136\n",
      "Gradient Descent(42/149): loss=0.40631272962557125\n",
      "Gradient Descent(43/149): loss=0.4060275171819081\n",
      "Gradient Descent(44/149): loss=0.4057488055864374\n",
      "Gradient Descent(45/149): loss=0.40547640054071044\n",
      "Gradient Descent(46/149): loss=0.40521011527575357\n",
      "Gradient Descent(47/149): loss=0.4049497700920188\n",
      "Gradient Descent(48/149): loss=0.40469519194505826\n",
      "Gradient Descent(49/149): loss=0.40444621407113873\n",
      "Gradient Descent(50/149): loss=0.40420267564780243\n",
      "Gradient Descent(51/149): loss=0.40396442148505307\n",
      "Gradient Descent(52/149): loss=0.4037313017434313\n",
      "Gradient Descent(53/149): loss=0.4035031716757464\n",
      "Gradient Descent(54/149): loss=0.40327989138966325\n",
      "Gradient Descent(55/149): loss=0.4030613256287183\n",
      "Gradient Descent(56/149): loss=0.4028473435696602\n",
      "Gradient Descent(57/149): loss=0.40263781863428943\n",
      "Gradient Descent(58/149): loss=0.40243262831420923\n",
      "Gradient Descent(59/149): loss=0.40223165400710864\n",
      "Gradient Descent(60/149): loss=0.4020347808633781\n",
      "Gradient Descent(61/149): loss=0.401841897642008\n",
      "Gradient Descent(62/149): loss=0.4016528965748598\n",
      "Gradient Descent(63/149): loss=0.40146767323850907\n",
      "Gradient Descent(64/149): loss=0.40128612643296174\n",
      "Gradient Descent(65/149): loss=0.40110815806663147\n",
      "Gradient Descent(66/149): loss=0.400933673047039\n",
      "Gradient Descent(67/149): loss=0.4007625791767587\n",
      "Gradient Descent(68/149): loss=0.40059478705419554\n",
      "Gradient Descent(69/149): loss=0.40043020997882045\n",
      "Gradient Descent(70/149): loss=0.4002687638605367\n",
      "Gradient Descent(71/149): loss=0.40011036713288795\n",
      "Gradient Descent(72/149): loss=0.3999549406698429\n",
      "Gradient Descent(73/149): loss=0.39980240770593\n",
      "Gradient Descent(74/149): loss=0.39965269375950907\n",
      "Gradient Descent(75/149): loss=0.39950572655899397\n",
      "Gradient Descent(76/149): loss=0.39936143597185797\n",
      "Gradient Descent(77/149): loss=0.3992197539362671\n",
      "Gradient Descent(78/149): loss=0.39908061439520215\n",
      "Gradient Descent(79/149): loss=0.39894395323294257\n",
      "Gradient Descent(80/149): loss=0.3988097082137968\n",
      "Gradient Descent(81/149): loss=0.3986778189229704\n",
      "Gradient Descent(82/149): loss=0.398548226709475\n",
      "Gradient Descent(83/149): loss=0.39842087463098685\n",
      "Gradient Descent(84/149): loss=0.39829570740057024\n",
      "Gradient Descent(85/149): loss=0.39817267133518697\n",
      "Gradient Descent(86/149): loss=0.3980517143059204\n",
      "Gradient Descent(87/149): loss=0.3979327856898428\n",
      "Gradient Descent(88/149): loss=0.39781583632346407\n",
      "Gradient Descent(89/149): loss=0.3977008184576996\n",
      "Gradient Descent(90/149): loss=0.3975876857143011\n",
      "Gradient Descent(91/149): loss=0.39747639304369636\n",
      "Gradient Descent(92/149): loss=0.3973668966841854\n",
      "Gradient Descent(93/149): loss=0.3972591541224468\n",
      "Gradient Descent(94/149): loss=0.3971531240553051\n",
      "Gradient Descent(95/149): loss=0.3970487663527182\n",
      "Gradient Descent(96/149): loss=0.39694604202193967\n",
      "Gradient Descent(97/149): loss=0.3968449131728186\n",
      "Gradient Descent(98/149): loss=0.3967453429841955\n",
      "Gradient Descent(99/149): loss=0.396647295671361\n",
      "Gradient Descent(100/149): loss=0.39655073645453814\n",
      "Gradient Descent(101/149): loss=0.39645563152835755\n",
      "Gradient Descent(102/149): loss=0.3963619480322901\n",
      "Gradient Descent(103/149): loss=0.39626965402200776\n",
      "Gradient Descent(104/149): loss=0.3961787184416421\n",
      "Gradient Descent(105/149): loss=0.39608911109690903\n",
      "Gradient Descent(106/149): loss=0.39600080262907567\n",
      "Gradient Descent(107/149): loss=0.39591376448973836\n",
      "Gradient Descent(108/149): loss=0.39582796891638833\n",
      "Gradient Descent(109/149): loss=0.39574338890873906\n",
      "Gradient Descent(110/149): loss=0.39565999820579006\n",
      "Gradient Descent(111/149): loss=0.3955777712636053\n",
      "Gradient Descent(112/149): loss=0.3954966832337822\n",
      "Gradient Descent(113/149): loss=0.39541670994258954\n",
      "Gradient Descent(114/149): loss=0.3953378278707541\n",
      "Gradient Descent(115/149): loss=0.3952600141338733\n",
      "Gradient Descent(116/149): loss=0.3951832464634368\n",
      "Gradient Descent(117/149): loss=0.3951075031884353\n",
      "Gradient Descent(118/149): loss=0.3950327632175401\n",
      "Gradient Descent(119/149): loss=0.39495900602183315\n",
      "Gradient Descent(120/149): loss=0.3948862116180734\n",
      "Gradient Descent(121/149): loss=0.3948143605524793\n",
      "Gradient Descent(122/149): loss=0.39474343388501304\n",
      "Gradient Descent(123/149): loss=0.3946734131741508\n",
      "Gradient Descent(124/149): loss=0.3946042804621229\n",
      "Gradient Descent(125/149): loss=0.39453601826060986\n",
      "Gradient Descent(126/149): loss=0.39446860953687984\n",
      "Gradient Descent(127/149): loss=0.3944020377003532\n",
      "Gradient Descent(128/149): loss=0.39433628658958136\n",
      "Gradient Descent(129/149): loss=0.39427134045962714\n",
      "Gradient Descent(130/149): loss=0.39420718396983284\n",
      "Gradient Descent(131/149): loss=0.3941438021719655\n",
      "Gradient Descent(132/149): loss=0.3940811804987266\n",
      "Gradient Descent(133/149): loss=0.39401930475261454\n",
      "Gradient Descent(134/149): loss=0.3939581610951301\n",
      "Gradient Descent(135/149): loss=0.3938977360363128\n",
      "Gradient Descent(136/149): loss=0.39383801642459804\n",
      "Gradient Descent(137/149): loss=0.3937789894369866\n",
      "Gradient Descent(138/149): loss=0.3937206425695143\n",
      "Gradient Descent(139/149): loss=0.39366296362801445\n",
      "Gradient Descent(140/149): loss=0.39360594071916205\n",
      "Gradient Descent(141/149): loss=0.3935495622417938\n",
      "Gradient Descent(142/149): loss=0.3934938168784916\n",
      "Gradient Descent(143/149): loss=0.3934386935874251\n",
      "Gradient Descent(144/149): loss=0.3933841815944421\n",
      "Gradient Descent(145/149): loss=0.39333027038540047\n",
      "Gradient Descent(146/149): loss=0.39327694969873533\n",
      "Gradient Descent(147/149): loss=0.39322420951825077\n",
      "Gradient Descent(148/149): loss=0.39317204006613277\n",
      "Gradient Descent(149/149): loss=0.39312043179617434\n"
     ]
    }
   ],
   "source": [
    "# Linear regression using gradient descent\n",
    "\n",
    "initial_w = np.zeros(tX.shape[1])\n",
    "max_iters = 150\n",
    "gamma = 0.085\n",
    "\n",
    "weights, loss = least_squares_GD(y_, standardize(x), initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least squares regression using normal equation\n",
    "weight, loss = least_squares(y, tX)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression using normal equations\n",
    "lambda_ = 1e-3\n",
    "weight, loss = ridge_regression(y, tX, lambda_)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'result.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

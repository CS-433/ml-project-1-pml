import numpy as np
from proj1_helpers import *
from helpers_data import *

#####################
def normalize(data):
    return (data-np.min(data, axis = 0))/(np.max(data, axis = 0)-np.min(data, axis = 0))

def standardize(data):
    return (data - np.average(data, axis = 0)) / (np.std(data, axis = 0))

#####################

##################### LOSSES
def compute_loss(y, tx, w):
    """Calculate the loss."""
    loss = ((y - tx.dot(w))**2).sum()/(2*len(y))   #MSE
    return loss

def logsig(x):
    '''Compute the log-sigmoid function component-wise based on MÃ¤chler, Martin
    paper which prevent NaN issue for instance generated by ln(exp(800)) for instance.'''
    logsig = np.zeros_like(x)
    index0 = x < -33
    logsig[index0] = x[index0]
    index1 = (x >= -33) & (x < -18)
    logsig[index1] = x[index1] - np.exp(x[index1])
    index2 = (x >= -18) & (x < 37)
    logsig[index2] = -np.log1p(np.exp(-x[index2]))
    index3 = x >= 37
    logsig[index3] = -np.exp(-x[index3])
    return logsig

def compute_loss_log(y, tx, w):
    '''Compute the loss function for a logistic model'''
    z = np.dot(tx, w)
    y = np.asarray(y)
    return np.mean((1 - y) * z - logsig(z))


##################### GRADIENTS
def compute_gradient(y, tx, w):
    """Compute the gradient."""
    grad = -tx.T.dot(y - tx.dot(w))/len(y)
    return grad

def compute_sigy(x, y):
    ''' Compute sig(x)-y composent-wise
    with sig(x)=1/(1+exp(-x))'''
    index = x < 0
    result = np.zeros_like(x)
    exp_x = np.exp(x[index])
    y_index = y[index]
    result[index] = ((1 - y_index) * exp_x - y_index) / (1 + exp_x)
    exp_nx = np.exp(-x[~index])
    y_nidx = y[~index]
    result[~index] = ((1 - y_nidx) - y_nidx * exp_nx) / (1 + exp_nx)
    return result

def compute_gradient_log(y, tx, w):
    z = tx.dot(w)
    s = compute_sigy(z, y)
    return tx.T.dot(s) / tx.shape[0]


##################### BATCH

def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):
    """
    Generate a minibatch iterator for a dataset.
    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')
    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.
    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.
    Example of use :
    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):
        <DO-SOMETHING>
    """
    data_size = len(y)

    if shuffle:
        shuffle_indices = np.random.permutation(np.arange(data_size))
        shuffled_y = y[shuffle_indices]
        shuffled_tx = tx[shuffle_indices]
    else:
        shuffled_y = y
        shuffled_tx = tx
    for batch_num in range(num_batches):
        start_index = batch_num * batch_size
        end_index = min((batch_num + 1) * batch_size, data_size)
        if start_index != end_index:
            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]



##################### METHODS
def least_squares_GD(y, tx, initial_w, max_iters, gamma):
    """Gradient descent algorithm."""
    w = initial_w
    for n_iter in range(max_iters):
        gradient = compute_gradient(y, tx, w)
        loss = compute_loss(y, tx, w)
        w = w - (gamma * gradient)
        print("Gradient Descent({bi}/{ti}): loss={l}".format(
              bi=n_iter, ti=max_iters - 1, l=loss))
    return w, loss


def least_squares_SGD(
        y, tx, initial_w, max_iters, gamma):
    """Stochastic gradient descent algorithm."""
    batch_size = 1
    w = initial_w
    for n_iter in range(max_iters):
        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):
            gradient = compute_gradient(minibatch_y, minibatch_tx, w)
            loss = compute_loss(minibatch_y, minibatch_tx, w)
            w = w - (gamma * gradient)
        print("Gradient Descent({bi}/{ti}): loss={l}".format(
              bi=n_iter, ti=max_iters - 1, l=loss))
    return w, loss


def least_squares(y, tx):
    """calculate the least squares solution."""
    w = np.linalg.solve(tx.T.dot(tx), tx.T.dot(y))
    mse = compute_loss(y, tx, w)
    return w, mse 


def ridge_regression(y, tx, lambda_):
    """implement ridge regression."""
    lambda_pr = lambda_ * 2 * len(y)
    if np.linalg.det(tx.T @ tx + lambda_pr * np.eye(tx.shape[1])) == 0:
        w= np.zeros((tx.shape[1], 1))
        loss= 1000
    else:
        w = np.linalg.solve(tx.T @ tx + lambda_pr * np.eye(tx.shape[1]), tx.T @ y)
        loss = compute_loss(y, tx, w)
    return w, loss



##################### LOGISTIC REGRESSION

def logistic_regression(y, tx, initial_w, max_iters, gamma):
    threshold = 1e-4
    w = initial_w
    loss_prev = 0

    for iter in range(max_iters):
        # get loss and update w.
        loss = compute_loss_log(y, tx, w)
        grad = compute_gradient_log(y, tx, w)
        w -= gamma * grad
        # log info
        if iter % 100 == 0:
            print("Current iteration={i}, loss={l}".format(i=iter, l=loss))
        # converge criterion
        if iter > 1 and np.abs(loss - loss_prev) < threshold:
            break  
        loss_prev = loss

    print("loss={l}".format(l=compute_loss_log(y, tx, w)))
    return w, loss  


def reg_logistic_regression(y, tx, lambda_, initial_w, max_iter, gamma):
    # init parameters
    threshold = 1e-5
    loss_prev = 0

    # build tx
    w = initial_w
    y = y.reshape((-1,1))

    # start the logistic regression
    for iter in range(max_iter):
        # get loss and update w.
        loss = compute_loss_log(y, tx, w) + lambda_ * np.squeeze(w.T.dot(w))
        gradient = compute_gradient_log(y, tx, w) + 2 * lambda_ * w
        w = w - gamma * gradient
        
        # log info
        # if iter % 100 == 0:
        #     print("Current iteration={i}, loss={l}".format(i=iter, l=loss))

        # converge criterion
        if loss < 0:
            print('NEGATIVE LOSS')
            break
        if iter > 1 and np.abs(loss - loss_prev) < threshold:
            break
        loss_prev = loss
    return loss, w



def grid_search(y, tX, function, log = False, k_fold = 4, degrees = range(1, 8), lambdas = np.logspace(-8, -1, 10)):
    # Ridge regression with K-fold
    k_indices = build_k_indices(y, k_fold)

    rmse_te_tmp = []
    BestLambdaForDeg=[]
    for index_degree, degree in enumerate(degrees):
        rmse_te_tmp2 = []
        for index_lambda, lambda_ in enumerate(lambdas):
            loss_te_tmp = 0
            for k in range(k_fold):
                _, loss_te, _ = cross_validation(y, tX, k_indices, k, degree, function, (lambda_,), log)
                loss_te_tmp = loss_te_tmp + loss_te
            rmse_te_tmp2.append(np.sqrt(2 * loss_te_tmp / k_fold))
        rmse_te_tmp.append(min(rmse_te_tmp2))
        BestLambdaForDeg.append(lambdas[np.argmin(rmse_te_tmp2)])
    BestDeg = degrees[np.argmin(rmse_te_tmp)]
    BestLambda = BestLambdaForDeg[np.argmin(rmse_te_tmp)]
    rmse_te = min(rmse_te_tmp)

    return rmse_te, BestDeg, BestLambda


def grid_search_for_log_reg(y, tX, log = False, k_fold = 4, degrees = range(1, 15), lambdas = np.logspace(-7, -1, 25), gammas = np.logspace(-11, -8, 25)):

    k_indices = build_k_indices(y, k_fold)

    rmse_te_tmp = np.empty((len(degrees), len(gammas),len(lambdas)))
    for index_degree, degree in enumerate(degrees):
        for index_gamma, gamma in enumerate(gammas):
            for index_lambda, lambda_ in enumerate(lambdas):
                loss_te_tmp = 0
                for k in range(k_fold):
                    _, loss_te, _ = cross_validation_log_len(y, tX, k_indices, k, degree, lambda_, gamma,log)
                    loss_te_tmp = loss_te_tmp + loss_te
                rmse_te_tmp[index_degree, index_gamma, index_lambda]= loss_te_tmp / k_fold
                print(rmse_te_tmp[index_degree, index_gamma, index_lambda])
            print("Done Lambda")
        print("Done Gamma")
    print("Done Deg")
    rmse_te = np.nanmin(rmse_te_tmp)
    # print(rmse_te_tmp.shape)
    # print(rmse_te_tmp[0,0,1])
    Ind_best_param = np.where(rmse_te_tmp == rmse_te)
    print(Ind_best_param)
    BestDeg = degrees[np.squeeze(Ind_best_param[0])]
    BestGamma = gammas[np.squeeze(Ind_best_param[1])]
    BestLambda = lambdas[np.squeeze(Ind_best_param[2])]
    return rmse_te, BestDeg, BestLambda, BestGamma


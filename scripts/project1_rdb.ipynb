{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n2/lkxf5xgn5qd0l88pblwqxc6h0000gn/T/ipykernel_37732/1938088011.py:11: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  corr= num/den\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 3, 5], [0, 6, 9], [1, 0, 2], [1, 3, 6], [1, 3, 18], [1, 3, 21], [1, 6, 18], [1, 6, 21], [1, 18, 21], [2, 0, 2], [2, 3, 9], [2, 4, 5], [2, 9, 21], [2, 9, 22], [2, 9, 28], [2, 21, 28], [2, 22, 28]]\n"
     ]
    }
   ],
   "source": [
    "#Check data\n",
    "tX_list, ids_list = separate_dataset(tX, ids)\n",
    "index=[]\n",
    "for seT in range(len(tX_list)-1):\n",
    "    for indexX, column in enumerate(tX_list[seT].T):\n",
    "        x_= np.mean(column)\n",
    "        for indexY, column2 in enumerate(tX_list[seT].T):\n",
    "            y_= np.mean(column2)\n",
    "            num=np.dot((column - x_),(column2 - y_))\n",
    "            den= np.sqrt(np.dot(column - x_,column - x_)*np.dot(column2 - y_,column2 - y_))\n",
    "            corr= num/den\n",
    "            if corr> 0.8 and indexX!= indexY and indexX< indexY:\n",
    "                index.append([seT,indexX, indexY])\n",
    "\n",
    "print(index)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "build_k_indices() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n2/lkxf5xgn5qd0l88pblwqxc6h0000gn/T/ipykernel_37732/1557179070.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mBestLambda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#x_tr, x_te, y_tr, y_te= split_data(tX, y, ratio_train, seed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mk_indices\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mbuild_k_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mrmse_te_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mBestLambdaForDeg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: build_k_indices() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# Ridge regression with K-fold\n",
    "k_fold= 4\n",
    "degrees = range(1, 3)\n",
    "lambdas = np.logspace(-4, 0, 30)\n",
    "seed = 1\n",
    "#ratio_train = 0.7\n",
    "\n",
    "rmse_te = 0\n",
    "BestDeg=0\n",
    "BestLambda=0\n",
    "#x_tr, x_te, y_tr, y_te= split_data(tX, y, ratio_train, seed)\n",
    "k_indices =build_k_indices(y, k_fold, seed)\n",
    "rmse_te_tmp = []\n",
    "BestLambdaForDeg=[]\n",
    "for index_degree, degree in enumerate(degrees):\n",
    "    rmse_te_tmp2 = []\n",
    "    for index_lambda, lambda_ in enumerate(lambdas):\n",
    "        loss_te_tmp=0\n",
    "        for k in range(k_fold):\n",
    "            _, loss_te, w=cross_validation(y, tX, k_indices, k, lambda_, degree)\n",
    "            loss_te_tmp= loss_te_tmp+loss_te\n",
    "        loss_te_tmp=loss_te_tmp/k_fold\n",
    "        rmse_te_tmp2.append(np.sqrt(2*loss_te_tmp))\n",
    "    rmse_te_tmp.append(min(rmse_te_tmp2))\n",
    "    BestLambdaForDeg.append(lambdas[np.argmin(rmse_te_tmp2)])\n",
    "BestDeg=degrees[np.argmin(rmse_te_tmp)]\n",
    "BestLambda= BestLambdaForDeg[np.argmin(rmse_te_tmp)]\n",
    "rmse_te=min(rmse_te_tmp)\n",
    "\n",
    "\n",
    "loss=rmse_te\n",
    "print(BestDeg)\n",
    "print(BestLambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33968680947709307"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Least squares regression using normal equation\n",
    "weight, loss = least_squares(y, tX)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33944686042508954"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ridge regression using normal equations\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "lambda_ = BestLambda\n",
    "tx=build_poly(tX,BestDeg)\n",
    "\n",
    "weights, loss = ridge_regression(y, tx, lambda_)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset part 0:\n",
      "     Best degree: 1\n",
      "     Best lambda: 3.359818286283781e-07\n",
      "     Loss: 0.7177556387376729\n",
      "Dataset part 1:\n",
      "     Best degree: 6\n",
      "     Best lambda: 0.00026366508987303583\n",
      "     Loss: 0.7903514696724503\n",
      "Dataset part 2:\n",
      "     Best degree: 5\n",
      "     Best lambda: 1e-07\n",
      "     Loss: 0.7625660276943759\n"
     ]
    }
   ],
   "source": [
    "#Importation of train and test data:\n",
    "\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "#Separate Data Set\n",
    "from implementations import *\n",
    "\n",
    "tX_list, ids_list, y_list = separate_dataset(tX, ids, y)\n",
    "tX_test_list, ids_test_list = separate_dataset(tX_test, ids_test)\n",
    "\n",
    "\n",
    "\n",
    "#Grid Search of param\n",
    "function = ridge_regression\n",
    "degree_vec = []\n",
    "lambda_vec = []\n",
    "for i in range(3):\n",
    "    print('Dataset part {l}:'.format(l = i))\n",
    "    rmse_te, BestDeg, BestLambda = grid_search(y_list[i], tX_list[i], function, True)\n",
    "    degree_vec.append(BestDeg)\n",
    "    lambda_vec.append(BestLambda)\n",
    "    print('     Best degree: {d}'.format(d = BestDeg))\n",
    "    print('     Best lambda: {m}'.format(m = BestLambda))\n",
    "    print('     Loss: {lo}'.format(lo = rmse_te))\n",
    "\n",
    "\n",
    "\n",
    "#Training\n",
    "weights_list = []\n",
    "loss_list = []\n",
    "mat_tX_test_list = []\n",
    "\n",
    "for i in range(3):\n",
    "    mat_tX, mat_tX_test = build_poly_log(tX_list[i], degree_vec[i], True, tX_test_list[i])\n",
    "    w, l = ridge_regression(y_list[i], mat_tX, lambda_vec[i])\n",
    "    weights_list.append(w)\n",
    "    loss_list.append(l)\n",
    "    mat_tX_test_list.append(mat_tX_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset part 0:\n",
      "Current iteration=0, loss=51940.29082807812\n",
      "Current iteration=100, loss=49220.02719025205\n",
      "Current iteration=200, loss=46554.10092824623\n",
      "Current iteration=300, loss=43941.333346120555\n",
      "Current iteration=400, loss=41380.51908231169\n",
      "Current iteration=500, loss=38870.43324301527\n",
      "Current iteration=600, loss=36409.83808719982\n",
      "Current iteration=700, loss=33997.48907809789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robindebalme/Documents/Cours EPFL MA1/Machine Learning/ml-project-1-pml/scripts/implementations.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  sigm = 1 / (1 + np.exp(-t))\n",
      "/Users/robindebalme/Documents/Cours EPFL MA1/Machine Learning/ml-project-1-pml/scripts/implementations.py:26: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=51940.29082807811\n",
      "Current iteration=100, loss=49198.31452897766\n",
      "Current iteration=200, loss=46511.04481751\n",
      "Current iteration=300, loss=43877.296312345396\n",
      "Current iteration=400, loss=41295.85668469437\n",
      "Current iteration=500, loss=38765.49384332092\n",
      "Current iteration=600, loss=36284.962661400365\n",
      "Current iteration=700, loss=33853.011058824035\n",
      "Current iteration=0, loss=51940.29082807813\n",
      "Current iteration=100, loss=49211.314715437904\n",
      "Current iteration=200, loss=46536.80303761198\n",
      "Current iteration=300, loss=43915.57851253695\n",
      "Current iteration=400, loss=41346.438143456246\n",
      "Current iteration=500, loss=38828.15949710209\n",
      "Current iteration=600, loss=36359.506730150744\n",
      "Current iteration=700, loss=33939.23626587777\n",
      "Current iteration=0, loss=51940.29082807811\n",
      "Current iteration=100, loss=49191.60812167902\n",
      "Current iteration=200, loss=46497.925125003254\n",
      "Current iteration=300, loss=43858.04480707411\n",
      "Current iteration=400, loss=41270.74209537753\n",
      "Current iteration=500, loss=38734.77137732863\n",
      "Current iteration=600, loss=36248.87353611918\n",
      "Current iteration=700, loss=33811.78231663864\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n2/lkxf5xgn5qd0l88pblwqxc6h0000gn/T/ipykernel_43926/2931439854.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dataset part {l}:'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mrmse_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBestDeg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBestLambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBestGamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search_for_log_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mdegree_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBestDeg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mlambda_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBestLambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Cours EPFL MA1/Machine Learning/ml-project-1-pml/scripts/implementations.py\u001b[0m in \u001b[0;36mgrid_search_for_log_reg\u001b[0;34m(y, tX, log, k_fold, degrees, lambdas, gammas)\u001b[0m\n\u001b[1;32m    336\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation_log_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m                     \u001b[0mloss_te_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_te_tmp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_te\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0mrmse_te_tmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_degree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_lambda\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss_te_tmp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0mrmse_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrmse_te_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "#Separate Data Set\n",
    "from implementations import *\n",
    "\n",
    "tX_list, ids_list, y_list = separate_dataset(tX, ids, y)\n",
    "tX_test_list, ids_test_list = separate_dataset(tX_test, ids_test)\n",
    "\n",
    "degree_vec = []\n",
    "lambda_vec = []\n",
    "gamma_vec = []\n",
    "for i in range(3):\n",
    "    print('Dataset part {l}:'.format(l = i))\n",
    "    rmse_te, BestDeg, BestLambda, BestGamma = grid_search_for_log_reg(y_list[i], tX_list[i], False)\n",
    "    degree_vec.append(BestDeg)\n",
    "    lambda_vec.append(BestLambda)\n",
    "    gamma_vec.append(BestGamma)\n",
    "    print('     Best degree: {d}'.format(d = BestDeg))\n",
    "    print('     Best lambda: {m}'.format(m = BestLambda))\n",
    "    print('     Best gamma: {g}'.format(g = BestGamma))\n",
    "    print('     Loss: {lo}'.format(lo = rmse_te))\n",
    "\n",
    "weights_list = []\n",
    "loss_list = []\n",
    "mat_tX_test_list = []\n",
    "max_iter= 800\n",
    "\n",
    "for i in range(3):\n",
    "    mat_tX, mat_tX_test = build_poly_log(tX_list[i], degree_vec[i], False, tX_test_list[i])\n",
    "    initial_w = np.zeros((mat_tX.shape[1], 1))\n",
    "    w, l = logistic_regression_penalized_gradient_descent(y_list[i], mat_tX, initial_w, max_iter, gamma_vec[i], lambda_vec[i])(y_list[i], mat_tX, lambda_vec[i])\n",
    "    weights_list.append(w)\n",
    "    loss_list.append(l)\n",
    "    mat_tX_test_list.append(mat_tX_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test_poly=build_poly(tX_test,BestDeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'result.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test_poly)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submission with separate data, grid search and ridge of log\n",
    "\n",
    "OUTPUT_PATH = 'result.csv' # TODO: fill in desired name of output file for submission\n",
    "\n",
    "# mat_tX_test_list = build_poly_separated(tX_test_list, 3)            #data augmentation\n",
    "y_pred_list = separated_eval(weights_list, mat_tX_test_list)        #prediction\n",
    "\n",
    "y_pred = np.concatenate((y_pred_list[0], y_pred_list[1], y_pred_list[2]))\n",
    "ids_test_sub = np.concatenate((ids_test_list[0], ids_test_list[1], ids_test_list[2]))\n",
    "\n",
    "create_csv_submission(ids_test_sub, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

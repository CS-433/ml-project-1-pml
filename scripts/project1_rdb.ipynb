{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73790, 18)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tX_list, ids_list = separate_dataset(tX, ids)\n",
    "print(tX_list[1].shape)\n",
    "\n",
    "\n",
    "alpha = 0.98\n",
    "for i in range(6):\n",
    "    dist_fr_mean = np.std(tX_list[i], axis = 0)/(np.sqrt(1-alpha))\n",
    "    mean = np.average(tX_list[i], axis = 0)\n",
    "    print(tX_list[i].shape)\n",
    "    tX_list[i] = np.where(np.abs(tX_list[i]-mean) > dist_fr_mean, np.nan,tX_list[i])\n",
    "    tX_list[i] = tX_list[i][~np.isnan(tX_list[i]).any(axis=1)]\n",
    "    print(tX_list[i].shape)\n",
    "\n",
    "\n",
    "# for i in range(1):\n",
    "#     a = np.quantile(tX_list[0], alpha/2, axis = 0)\n",
    "#     b = np.quantile(tX_list[0], 1-(alpha/2), axis = 0)\n",
    "#     tX_list[0] = np.where(np.logical_or(tX_list[0]<a, tX_list[0]>b), np.nan,tX_list[0])\n",
    "#     print((tX_list[0][~np.isnan(tX_list[0]).any(axis=1)]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 2, 4], [1, 0, 2], [1, 3, 5], [2, 17, 20], [3, 0, 2], [3, 3, 6], [3, 3, 18], [3, 3, 21], [3, 6, 18], [3, 6, 21], [3, 18, 21], [4, 8, 27], [4, 21, 27], [5, 0, 2], [5, 9, 21], [5, 9, 22], [5, 9, 28], [5, 21, 28], [5, 22, 28]]\n"
     ]
    }
   ],
   "source": [
    "#Check data\n",
    "tX_list, ids_list = separate_dataset(tX, ids)\n",
    "index=[]\n",
    "for seT in range(len(tX_list)):\n",
    "    for indexX, column in enumerate(tX_list[seT].T):\n",
    "        x_= np.mean(column)\n",
    "        for indexY, column2 in enumerate(tX_list[seT].T):\n",
    "            y_= np.mean(column2)\n",
    "            num=np.dot((column - x_),(column2 - y_))\n",
    "            den= np.sqrt(np.dot(column - x_,column - x_)*np.dot(column2 - y_,column2 - y_))\n",
    "            corr= num/den\n",
    "            if corr> 0.85 and indexX!= indexY and indexX< indexY:\n",
    "                index.append([seT,indexX, indexY])\n",
    "\n",
    "print(index)\n",
    "      \n",
    "def datamodif(tX_list, log=False):\n",
    "    tX_list[0]=np.delete(tX_list[0], [4], axis=1)\n",
    "    print(tX_list[0].shape)\n",
    "    long_tail = [0, 1, 2, 3, 4, 5, 6, 7, 10, 13, 15]\n",
    "    tX_list[0][:, long_tail]= np.sqrt(tX_list[0][:, long_tail])\n",
    "    if log:\n",
    "        normalize(tX_list[0])\n",
    "    else:\n",
    "        standardize(tX_list[0])\n",
    "\n",
    "    tX_list[1]=np.delete(tX_list[1], [2,5], axis=1)\n",
    "    print(tX_list[1].shape)\n",
    "    long_tail = [0, 1, 4, 5, 7, 10, 13, 7, 13, 15]\n",
    "    tX_list[1][:, long_tail]= np.sqrt(tX_list[1][:, long_tail])\n",
    "    #tX_list[1][:, 6]= np.where(tX_list[1][:, 6]> 0.5, 1, 0)\n",
    "    if log:\n",
    "        normalize(tX_list[1])\n",
    "    else:\n",
    "        standardize(tX_list[1])\n",
    "\n",
    "    tX_list[2]=np.delete(tX_list[2], [20], axis=1)\n",
    "    print(tX_list[2].shape)\n",
    "    long_tail = [0, 1, 2, 3, 4, 5, 6, 8, 11, 14, 16, 17]\n",
    "    tX_list[2][:, long_tail]= np.sqrt(tX_list[2][:, long_tail])\n",
    "    #tX_list[2][:, 7]= np.where(tX_list[2][:, 7]> 0.5, 1, 0)\n",
    "    if log:\n",
    "        normalize(tX_list[2])\n",
    "    else:\n",
    "        standardize(tX_list[2])\n",
    "\n",
    "    tX_list[3]=np.delete(tX_list[3], [3,6,18], axis=1)\n",
    "    print(tX_list[3].shape)\n",
    "    long_tail = [0, 1, 2, 3, 4, 5, 7, 10, 13, 15, 18]\n",
    "    tX_list[3][:, long_tail]= np.sqrt(tX_list[3][:, long_tail])\n",
    "    #tX_list[3][:, 6]= np.where(tX_list[3][:, 6]> 0.5, 1, 0)\n",
    "    if log:\n",
    "        normalize(tX_list[3])\n",
    "    else:\n",
    "        standardize(tX_list[3])\n",
    "\n",
    "    tX_list[4]=np.delete(tX_list[4], [27], axis=1)\n",
    "    print(tX_list[4].shape)\n",
    "    long_tail = [0, 1, 2, 3, 4, 6, 7, 8, 9, 12, 15, 18, 20, 21]\n",
    "    tX_list[4][:, long_tail]= np.sqrt(tX_list[4][:, long_tail])\n",
    "    #tX_list[4][:, 11]= np.where(tX_list[4][:, 11]> 0.5, 1, 0)\n",
    "    #tX_list[4][:, 10]= np.where(tX_list[4][:, 10]> 0.5, 1, 0)\n",
    "    if log:\n",
    "        normalize(tX_list[4])\n",
    "    else:\n",
    "        standardize(tX_list[4])\n",
    "\n",
    "    tX_list[5]=np.delete(tX_list[5], [2,9,28], axis=1)\n",
    "    print(tX_list[5].shape)\n",
    "    long_tail = [0, 1, 2, 3, 4, 6, 7, 8, 11, 14, 17, 19, 20, 23]\n",
    "    tX_list[5][:, long_tail]= np.sqrt(tX_list[5][:, long_tail])\n",
    "    #tX_list[5][:, 9]= np.where(tX_list[5][:, 9]> 0.5, 1, 0)\n",
    "    #tX_list[5][:, 10]= np.where(tX_list[5][:, 10]> 0.5, 1, 0)\n",
    "    if log:\n",
    "        normalize(tX_list[5])\n",
    "    else:\n",
    "        standardize(tX_list[5])\n",
    "\n",
    "    return tX_list\n",
    "\n",
    "#import seaborn as sns\n",
    "#for indexX, column in enumerate(tX_list[5].T):\n",
    "#    sns.displot(column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26123, 16)\n",
      "(73790, 16)\n",
      "(7562, 20)\n",
      "(69982, 19)\n",
      "(4429, 27)\n",
      "(68114, 26)\n",
      "(59263, 16)\n",
      "(168195, 16)\n",
      "(17243, 20)\n",
      "(158095, 19)\n",
      "(9982, 27)\n",
      "(155460, 26)\n"
     ]
    }
   ],
   "source": [
    "#Load Data\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "from implementations import *\n",
    "\n",
    "tX_list, ids_list, y_list = separate_dataset(tX, ids, y)\n",
    "tX_test_list, ids_test_list = separate_dataset(tX_test, ids_test)\n",
    "\n",
    "tX_list = datamodif(tX_list)\n",
    "tX_test_list = datamodif(tX_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset part 0:\n",
      "     Best degree: 5\n",
      "     Best lambda: 6.951927961775591e-13\n",
      "     Loss: 0.41281470964633366\n",
      "Dataset part 1:\n",
      "     Best degree: 19\n",
      "     Best lambda: 2.06913808111479e-07\n",
      "     Loss: 0.7616515593061501\n",
      "Dataset part 2:\n",
      "     Best degree: 4\n",
      "     Best lambda: 7.847599703514607e-08\n",
      "     Loss: 0.5231766428569052\n",
      "Dataset part 3:\n",
      "     Best degree: 7\n",
      "     Best lambda: 3.359818286283788e-11\n",
      "     Loss: 0.8025224676686908\n",
      "Dataset part 4:\n",
      "     Best degree: 9\n",
      "     Best lambda: 1.43844988828766e-06\n",
      "     Loss: 0.5451902719212371\n",
      "Dataset part 5:\n",
      "     Best degree: 7\n",
      "     Best lambda: 2.6366508987303554e-13\n",
      "     Loss: 0.7539102873274455\n"
     ]
    }
   ],
   "source": [
    "#Grid Search Ridge\n",
    "\n",
    "function = ridge_regression\n",
    "degree_vec = []\n",
    "lambda_vec = []\n",
    "\n",
    "\n",
    "for i in range(6):\n",
    "    print('Dataset part {l}:'.format(l = i))\n",
    "    rmse_te, BestDeg, BestLambda, _ = grid_search(y_list[i], tX_list[i], ridge_regression, log = False, k_fold = 4, degrees = range(1, 20), lambdas = np.logspace(-13, -5, 20), gammas = np.arange(1))\n",
    "    degree_vec.append(BestDeg)\n",
    "    lambda_vec.append(BestLambda)\n",
    "    print('     Best degree: {d}'.format(d = BestDeg))\n",
    "    print('     Best lambda: {m}'.format(m = BestLambda))\n",
    "    print('     Loss: {lo}'.format(lo = rmse_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "\n",
    "weights_list = []\n",
    "loss_list = []\n",
    "mat_tX_test_list = []\n",
    "\n",
    "for i in range(6):\n",
    "    mat_tX, mat_tX_test = build_poly_log(tX_list[i], degree_vec[i], False, tX_test_list[i])\n",
    "    w, l = ridge_regression(y_list[i], mat_tX, lambda_vec[i])\n",
    "    weights_list.append(w)\n",
    "    loss_list.append(l)\n",
    "    mat_tX_test_list.append(mat_tX_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval\n",
    "\n",
    "y_pred_list = separated_eval(weights_list, mat_tX_test_list, False) \n",
    "\n",
    "y_pred = np.concatenate((y_pred_list[0], y_pred_list[1], y_pred_list[2], y_pred_list[3], y_pred_list[4], y_pred_list[5]))\n",
    "ids_test_sub = np.concatenate((ids_test_list[0], ids_test_list[1], ids_test_list[2], ids_test_list[3], ids_test_list[4], ids_test_list[5]))\n",
    "\n",
    "OUTPUT_PATH = 'result.csv'\n",
    "create_csv_submission(ids_test_sub, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26123, 16)\n",
      "(73790, 16)\n",
      "(7562, 20)\n",
      "(69982, 19)\n",
      "(4429, 27)\n",
      "(68114, 26)\n",
      "(59263, 16)\n",
      "(168195, 16)\n",
      "(17243, 20)\n",
      "(158095, 19)\n",
      "(9982, 27)\n",
      "(155460, 26)\n"
     ]
    }
   ],
   "source": [
    "#Load Data for Log\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "from implementations import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset part 0:\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n2/lkxf5xgn5qd0l88pblwqxc6h0000gn/T/ipykernel_73070/527485971.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dataset part {l}:'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mrmse_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBestDeg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBestLambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBestGamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambdas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgammas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdegree_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBestDeg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlambda_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBestLambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Cours EPFL MA1/Machine Learning/ml-project-1-pml/scripts/implementations.py\u001b[0m in \u001b[0;36mgrid_search\u001b[0;34m(y, tX, function, log, k_fold, degrees, lambdas, gammas)\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0mloss_te_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m                     \u001b[0mloss_te_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_te_tmp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_te\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0mrmse_te_tmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_degree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_lambda\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss_te_tmp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Cours EPFL MA1/Machine Learning/ml-project-1-pml/scripts/implementations.py\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(y, x, k_indices, k, degree, function, args, log)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0minitial_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr_poly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mloss_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tr_poly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0mloss_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_te_poly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Cours EPFL MA1/Machine Learning/ml-project-1-pml/scripts/implementations.py\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[0;34m(y, tx, initial_w, max_iter, lambda_, gamma)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m# get loss and update w.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_gradient_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Cours EPFL MA1/Machine Learning/ml-project-1-pml/scripts/implementations.py\u001b[0m in \u001b[0;36mcompute_gradient_log\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_gradient_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_sigy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "function = reg_logistic_regression\n",
    "degree_vec = []\n",
    "lambda_vec = []\n",
    "gamma_vec= []\n",
    "\n",
    "for i in range(6):\n",
    "    print('Dataset part {l}:'.format(l = i))\n",
    "    rmse_te, BestDeg, BestLambda, BestGamma = grid_search(y_list[i], tX_list[i], reg_logistic_regression, log = False, k_fold = 4, degrees = range(1,3), lambdas = np.logspace(-15, -6, 8), gammas = np.logspace(-4, 0, 5))\n",
    "    degree_vec.append(BestDeg)\n",
    "    lambda_vec.append(BestLambda)\n",
    "    gamma_vec.append(BestGamma)\n",
    "    print('     Best degree: {d}'.format(d = BestDeg))\n",
    "    print('     Best lambda: {m}'.format(m = BestLambda))\n",
    "    print('     Best gamma: {g}'.format(g = BestGamma))\n",
    "    print('     Loss: {lo}'.format(lo = rmse_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n",
      "treshold\n"
     ]
    }
   ],
   "source": [
    "#Train Logistic\n",
    "from implementations import *\n",
    "\n",
    "weights_list = []\n",
    "loss_list = []\n",
    "mat_tX_test_list = []\n",
    "max_iter= 5000\n",
    "\n",
    "for i in range(6):\n",
    "    initial_w= np.zeros((tX_list[i].shape[1], 1))\n",
    "    mat_tX, mat_tX_test = build_poly_log(tX_list[i], degree_vec[i], False, tX_test_list[i])\n",
    "    l, w = reg_logistic_regression(y_list[i], tX_list[i], initial_w, max_iter, lambda_vec[i], gamma_vec[i])\n",
    "    weights_list.append(w)\n",
    "    loss_list.append(l)\n",
    "    mat_tX_test_list.append(mat_tX_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Logistic\n",
    "\n",
    "y_pred_list = separated_eval(weights_list, mat_tX_test_list, True) \n",
    "\n",
    "y_pred = np.concatenate((y_pred_list[0], y_pred_list[1], y_pred_list[2], y_pred_list[3], y_pred_list[4], y_pred_list[5]))\n",
    "ids_test_sub = np.concatenate((ids_test_list[0], ids_test_list[1], ids_test_list[2], ids_test_list[3], ids_test_list[4], ids_test_list[5]))\n",
    "\n",
    "OUTPUT_PATH = 'result.csv'\n",
    "create_csv_submission(ids_test_sub, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset part 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/linalg/linalg.py:2158: RuntimeWarning: overflow encountered in det\n",
      "  r = _umath_linalg.det(a, signature=signature)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Best degree: 1\n",
      "     Best lambda: 0.0008733261623828437\n",
      "     Loss: 0.7380547288266697\n",
      "Dataset part 1:\n",
      "     Best degree: 5\n",
      "     Best lambda: 0.024118646996409948\n",
      "     Loss: 0.8058598609702398\n",
      "Dataset part 2:\n",
      "     Best degree: 4\n",
      "     Best lambda: 0.005817091329374358\n",
      "     Loss: 0.7826367955518109\n"
     ]
    }
   ],
   "source": [
    "#Importation of train and test data:\n",
    "\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "#Separate Data Set\n",
    "from implementations import *\n",
    "\n",
    "tX_list, ids_list, y_list = separate_dataset(tX, ids, y)\n",
    "tX_test_list, ids_test_list = separate_dataset(tX_test, ids_test)\n",
    "\n",
    "\n",
    "\n",
    "#Grid Search of param\n",
    "function = ridge_regression\n",
    "degree_vec = []\n",
    "lambda_vec = []\n",
    "for i in range(3):\n",
    "    print('Dataset part {l}:'.format(l = i))\n",
    "    rmse_te, BestDeg, BestLambda = grid_search(y_list[i], tX_list[i], function, True)\n",
    "    degree_vec.append(BestDeg)\n",
    "    lambda_vec.append(BestLambda)\n",
    "    print('     Best degree: {d}'.format(d = BestDeg))\n",
    "    print('     Best lambda: {m}'.format(m = BestLambda))\n",
    "    print('     Loss: {lo}'.format(lo = rmse_te))\n",
    "\n",
    "\n",
    "\n",
    "#Training\n",
    "weights_list = []\n",
    "loss_list = []\n",
    "mat_tX_test_list = []\n",
    "\n",
    "for i in range(3):\n",
    "    mat_tX, mat_tX_test = build_poly_log(tX_list[i], degree_vec[i], True, tX_test_list[i])\n",
    "    w, l = ridge_regression(y_list[i], mat_tX, lambda_vec[i])\n",
    "    weights_list.append(w)\n",
    "    loss_list.append(l)\n",
    "    mat_tX_test_list.append(mat_tX_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset part 0:\n",
      "Done Lambda\n",
      "Done Lambda\n",
      "Done Lambda\n",
      "Done Lambda\n",
      "Done Lambda\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n2/lkxf5xgn5qd0l88pblwqxc6h0000gn/T/ipykernel_64038/3558008276.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dataset part {l}:'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mrmse_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBestDeg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBestLambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBestGamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search_for_log_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mdegree_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBestDeg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mlambda_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBestLambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Cours EPFL MA1/Machine Learning/ml-project-1-pml/scripts/implementations.py\u001b[0m in \u001b[0;36mgrid_search_for_log_reg\u001b[0;34m(y, tX, log, k_fold, degrees, lambdas, gammas)\u001b[0m\n\u001b[1;32m    337\u001b[0m                 \u001b[0mloss_te_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation_log_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m                     \u001b[0mloss_te_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_te_tmp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_te\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mrmse_te_tmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_degree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_lambda\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_te_tmp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Cours EPFL MA1/Machine Learning/ml-project-1-pml/scripts/implementations.py\u001b[0m in \u001b[0;36mcross_validation_log_len\u001b[0;34m(y, x, k_indices, k, degree, lambda_, gamma, log)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0minitial_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr_poly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m     \u001b[0mloss_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression_penalized_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tr_poly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Cours EPFL MA1/Machine Learning/ml-project-1-pml/scripts/implementations.py\u001b[0m in \u001b[0;36mlogistic_regression_penalized_gradient_descent\u001b[0;34m(y, tx, initial_w, max_iter, gamma, lambda_)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;31m# get loss and update w.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_by_penalized_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;31m# log info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m#if iter % 100 == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Cours EPFL MA1/Machine Learning/ml-project-1-pml/scripts/implementations.py\u001b[0m in \u001b[0;36mlearning_by_penalized_gradient\u001b[0;34m(y, tx, w, gamma, lambda_)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mupdated\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \"\"\"\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpenalized_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Cours EPFL MA1/Machine Learning/ml-project-1-pml/scripts/implementations.py\u001b[0m in \u001b[0;36mpenalized_logistic_regression\u001b[0;34m(y, tx, w, lambda_)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpenalized_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;34m\"\"\"return the loss, gradient\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Cours EPFL MA1/Machine Learning/ml-project-1-pml/scripts/implementations.py\u001b[0m in \u001b[0;36mcross_entropy_loss\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;34m\"\"\"compute the loss: negative log likelihood.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Cours EPFL MA1/Machine Learning/ml-project-1-pml/scripts/implementations.py\u001b[0m in \u001b[0;36msigmoid\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m\"\"\"apply the sigmoid function on t.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0msigm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msigm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "#Separate Data Set\n",
    "from implementations import *\n",
    "\n",
    "tX_list, ids_list, y_list = separate_dataset(tX, ids, y)\n",
    "tX_test_list, ids_test_list = separate_dataset(tX_test, ids_test)\n",
    "\n",
    "degree_vec = []\n",
    "lambda_vec = []\n",
    "gamma_vec = []\n",
    "for i in range(3):\n",
    "    print('Dataset part {l}:'.format(l = i))\n",
    "    rmse_te, BestDeg, BestLambda, BestGamma = grid_search_for_log_reg(y_list[i], tX_list[i], False)\n",
    "    degree_vec.append(BestDeg)\n",
    "    lambda_vec.append(BestLambda)\n",
    "    gamma_vec.append(BestGamma)\n",
    "    print('     Best degree: {d}'.format(d = BestDeg))\n",
    "    print('     Best lambda: {m}'.format(m = BestLambda))\n",
    "    print('     Best gamma: {g}'.format(g = BestGamma))\n",
    "    print('     Loss: {lo}'.format(lo = rmse_te))\n",
    "\n",
    "weights_list = []\n",
    "loss_list = []\n",
    "mat_tX_test_list = []\n",
    "max_iter= 10000\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    mat_tX, mat_tX_test = build_poly_log(tX_list[i], degree_vec[i], False, tX_test_list[i])\n",
    "    initial_w = np.zeros((mat_tX.shape[1], 1))\n",
    "    l, w = logistic_regression_penalized_gradient_descent(y_list[i], mat_tX, initial_w, max_iter, gamma_vec[i], lambda_vec[i])\n",
    "    weights_list.append(w)\n",
    "    loss_list.append(l)\n",
    "    mat_tX_test_list.append(mat_tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n",
      "Data separated\n",
      "loop\n",
      "-64730.23741869711\n",
      "loop\n",
      "-63383.06999684557\n",
      "loop\n",
      "-181.017636445889\n"
     ]
    }
   ],
   "source": [
    "#Log Reg Pen\n",
    "\n",
    "\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "print(\"Data loaded\")\n",
    "\n",
    "\n",
    "#Separate Data Set\n",
    "from implementations import *\n",
    "\n",
    "tX_list, ids_list, y_list = separate_dataset(tX, ids, y)\n",
    "tX_test_list, ids_test_list = separate_dataset(tX_test, ids_test)\n",
    "print(\"Data separated\")\n",
    "\n",
    "degree_vec = [1 , 4 , 5]\n",
    "lambda_vec = [1e-05 , 1e-04 , 1e-04]\n",
    "gamma_vec = [8.65e-10 , 1.9e-9, 1e-9]\n",
    "\n",
    "weights_list = []\n",
    "loss_list = []\n",
    "mat_tX_test_list = []\n",
    "max_iter= 10000\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"loop\")\n",
    "    mat_tX, mat_tX_test = build_poly_log(tX_list[i], degree_vec[i], False, tX_test_list[i])\n",
    "    initial_w = np.zeros((mat_tX.shape[1], 1))\n",
    "    l, w = logistic_regression_penalized_gradient_descent(y_list[i], mat_tX, initial_w, max_iter, gamma_vec[i], lambda_vec[i])\n",
    "    print(l)\n",
    "    weights_list.append(w)\n",
    "    loss_list.append(l)\n",
    "    mat_tX_test_list.append(mat_tX_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test_poly=build_poly(tX_test,BestDeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'result.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test_poly)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submission with separate data, grid search and ridge of log\n",
    "\n",
    "OUTPUT_PATH = 'result.csv' # TODO: fill in desired name of output file for submission\n",
    "\n",
    "# mat_tX_test_list = build_poly_separated(tX_test_list, 3)            #data augmentation\n",
    "y_pred_list = separated_eval(weights_list, mat_tX_test_list)        #prediction\n",
    "\n",
    "y_pred = np.concatenate((y_pred_list[0], y_pred_list[1], y_pred_list[2]))\n",
    "ids_test_sub = np.concatenate((ids_test_list[0], ids_test_list[1], ids_test_list[2]))\n",
    "\n",
    "create_csv_submission(ids_test_sub, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
